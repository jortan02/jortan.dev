user_input,retrieved_contexts,response,reference,context_precision,context_recall,faithfulness,answer_relevancy
What is Jordan's educational background?,[],Jordan earned both his bachelor's and master's degrees in Computer Science from the University of Utah.,Jordan earned both his bachelor's and master's degrees in Computer Science from the University of Utah.,0.0,0.0,0.0,0.8131244858147273
What kind of roles is Jordan interested in?,[],"Jordan is interested in roles involving machine learning, full-stack development, and research-driven product work.","Jordan is interested in roles involving machine learning, full-stack development, and research-driven product work.",0.0,0.0,0.0,0.9909192111995067
How can I get in touch with Jordan?,[],The best way to contact Jordan is through his LinkedIn profile.,The best way to contact Jordan is through his LinkedIn profile.,0.0,0.0,0.0,0.9472029364375985
What was the final test accuracy for the Convolutional Neural Network project?,"['\npublished: true\ntitle: Convolutional Neural Network\ndescription: Implementation of a complete CNN architecture using only NumPy for image classification on CIFAR-10 dataset.\ndate: 2023-10-08\ncategory: Project\nskills: [Python, NumPy]\n\nOverview\nThis coursework project for CS 5353 (Deep Learning) demonstrates a comprehensive understanding of neural network fundamentals through the implementation of a complete convolutional neural network (CNN) from scratch using only NumPy. The project focuses on the mathematical principles behind CNNs while achieving competitive results on the CIFAR-10 image classification dataset without relying on deep learning libraries.\nKey Achievements\n\nAchieved 68.7% test accuracy on CIFAR-10\nSurpassed the benchmark accuracy of 62%\nBuilt all core CNN components from first principles\nSuccessfully trained a multi-layer model using only NumPy\n\nTechnical Implementation\n\nImplemented convolutional layers with custom kernels\nDeveloped max pooling layers for downsampling\nCreated batch normalization for training stability\nBuilt ReLU activation and softmax output layers\nDesigned fully connected layers for classification\nWrote backpropagation logic for all components\n', ""\npublished: true\ntitle: Replicating Partitioned Learned Bloom Filter\ndescription: Implementation of Vaidya et al.'s PLBF for significant space efficiency improvements to traditional bloom filters.\ndate: 2024-03-23\ncategory: Project\nskills: [Python, pandas, NumPy, scikit-learn]\n\nOverview\nThis coursework project for CS 4964 (Manage Data for & with ML) replicates and evaluates the Partitioned Learned Bloom Filter (PLBF) proposed by Vaidya et al. The PLBF architecture integrates machine learning models into Bloom Filters to reduce memory usage while preserving filtering performance. This implementation explores how different classifiers impact accuracy, space efficiency, and latency across varying filter designs and datasets.\nKey Findings\n\nTraditional Bloom Filters provided the highest accuracy and fastest queries but consumed the most memory\nLearned Bloom Filters using perceptrons significantly reduced memory usage with minimal accuracy loss\nMore complex models like neural nets and ensembles improved accuracy but increased latency\nClassifier choice enables tuning of space-performance trade-offs for specific applications\n\nTechnical Implementation\n\nImplemented a traditional Bloom Filter for baseline comparison\nTrained ML models including Perceptron, Bagged Perceptron, Random Forest, and Neural Network\nPartitioned data and integrated model predictions into filtering logic\nBuilt preprocessing and evaluation pipelines for the URL and EMBER datasets\nMeasured memory usage, false positive rates, query latency, and training time\n"", '\npublished: true\ntitle: Selective Unlearning of Training Data Without Complete Retraining\ndescription: A research project exploring efficient machine unlearning techniques using ensemble models to protect user privacy while maintaining model performance.\ndate: 2023-06-28\ncategory: Research\nskills: [Python, PyTorch]\nurl: https://docs.google.com/presentation/d/1ZiuXadZMVvYtrFtaREbwJNeWGyYnZwKc-gf4cEZRP-Y/edit?usp=sharing\nrepository: machine_unlearning\n\nOverview\nThis NSF-sponsored research project investigates machine unlearning, the process of removing the influence of specific data points from trained machine learning models. This is essential for privacy-preserving systems where users may request their data to be deleted, and retraining a model from scratch is inefficient or impractical.\nTo address this, we propose an ensemble-based method: a collection of models, each trained on random subsets of the dataset. When unlearning is required, only the affected models are replaced, avoiding full retraining and enabling continual learning.\nKey Findings\n\nEnsemble models trained on randomized data subsets can support selective unlearning by retraining only a subset of the ensemble\nAchieved better validation loss and accuracy than a larger monolithic ResNet-152 model despite using fewer total parameters\nThe system supports online-style learning, where data is introduced incrementally and unlearning requests can be processed efficiently\nDemonstrated that privacy compliance and model performance need not be mutually exclusive\n\nTechnical Implementation\n\nArchitecture: Ensemble of five ResNet-18 models (11.4M parameters each)\nBaseline Comparison: Single ResNet-152 (58.5M parameters) without unlearning\nTraining Strategy:\nModels trained on chunks of CIFAR-10 in sequence\nEach model receives a random subset of each chunk\nWhen data must be unlearned, models that trained on it are marked invalid\nInvalid models are replaced and retrained on the latest available data\nEvaluation:\nValidation accuracy and loss tracked after each training phase\nVisualizations highlight ensemble progress vs. baseline\n', ""\npublished: true\ntitle: Replicating SelfExplain Architecture\ndescription: Implementation and evaluation of Rajagopal et al.'s SelfExplain architecture for interpretable text classification with concept-level explanations.\ndate: 2023-11-30\ncategory: Project\nskills: [Python, pandas, PyTorch, HuggingFace]\n\nOverview\nA coursework project for CS 6957 (NLP with Deep Learning), focused on implementing core components of Rajagopal et al.’s SelfExplain architecture. The model produces interpretable predictions by identifying constituent concepts from input sentences and measuring their influence on classification outcomes. Concepts were extracted using parse trees and embedded in the training data to enable explanation-driven classification using a RoBERTa encoder and a Locally Interpretable Layer (LIL).\nKey Achievements\n\nReplicated the SelfExplain concept attribution strategy without building a full training pipeline\nAchieved 94.8% validation accuracy on the SST2 sentiment classification task\nExtracted interpretable concept spans from parse trees and embedded them into input features\nMeasured concept influence using score differences with masked concept representations\n\nTechnical Implementation\n\nGenerated constituency parse trees to identify phrase-level concepts\nEmbedded extracted spans into model input for interpretability\nBuilt a custom PyTorch model combining RoBERTa with a Locally Interpretable Layer (LIL)\nMeasured the influence of each concept by masking and comparing prediction deltas\nEvaluated interpretability through concept-level attribution and sparsity analysis\n"", ""\npublished: true\ntitle: Examining Compositional Behavior in Large Language Models\ndescription: A bachelor's thesis project investigating how large language models respond to in-context subtask prompting, with a focus on model behavior and prompting strategy effectiveness.\ndate: 2023-09-24\ncategory: Research\nskills: [Python, pandas, matplotlib, HuggingFace]\nurl: https://docs.google.com/presentation/d/1E8iCQb2ft1BG55gx3ug1KR_GFp9a8woTEknGuJlETbA/edit?usp=sharing\nrepository: compositional_task_evaluation\n\nOverview\nMy bachelor's thesis, supervised by Professor Vivek Srikumar, explores the compositional capabilities of large language models (LLMs). The project investigates whether in-context examples of subtasks—rather than complete tasks—can improve model performance in compositional reasoning scenarios. The research focused on tasks involving multi-digit multiplication and word list operations, analyzed under various prompting conditions using few-shot setups.\nKey Findings\n\nSubtask-based few-shot prompts showed limited or inconsistent gains\nFlan-T5 struggled with list-based operations more than expected\nPerformance was highly sensitive to prompt format and example selection\nModels exhibited task-specific reasoning gaps, even with relevant examples\n\nTechnical Implementation\n\nIndependently implemented all experiments and research infrastructure\nManually crafted and iterated on dozens of in-context prompt formats\nDesigned a suite of compositional tasks for systematic testing\nCreated Slurm scripts for efficient job scheduling across runs\nDeveloped comparison tools to measure accuracy and stability across models\nVisualized model performance trends using matplotlib\n\nResearch Components\n\nPrompt engineering: manual experimentation with few-shot examples\nData pipeline for generating task variants and model inputs\nEvaluation framework for task-specific performance metrics\nCross-model and cross-prompt comparisons to identify behavior patterns\nSynthesis of results for academic presentation and final thesis report\n""]","The final test accuracy for the Convolutional Neural Network project was **68.7%** on the CIFAR-10 dataset. This achievement surpassed the benchmark accuracy of 62%. The project involved implementing a complete CNN architecture from scratch using only NumPy, focusing on the mathematical principles behind CNNs.","The project achieved a test accuracy of 68.7% on the CIFAR-10 dataset, surpassing the benchmark of 62%.",0.9999999999,1.0,1.0,0.9410765472658994
"In the Selective Unlearning project, how was the ensemble model structured?","['\npublished: true\ntitle: Selective Unlearning of Training Data Without Complete Retraining\ndescription: A research project exploring efficient machine unlearning techniques using ensemble models to protect user privacy while maintaining model performance.\ndate: 2023-06-28\ncategory: Research\nskills: [Python, PyTorch]\nurl: https://docs.google.com/presentation/d/1ZiuXadZMVvYtrFtaREbwJNeWGyYnZwKc-gf4cEZRP-Y/edit?usp=sharing\nrepository: machine_unlearning\n\nOverview\nThis NSF-sponsored research project investigates machine unlearning, the process of removing the influence of specific data points from trained machine learning models. This is essential for privacy-preserving systems where users may request their data to be deleted, and retraining a model from scratch is inefficient or impractical.\nTo address this, we propose an ensemble-based method: a collection of models, each trained on random subsets of the dataset. When unlearning is required, only the affected models are replaced, avoiding full retraining and enabling continual learning.\nKey Findings\n\nEnsemble models trained on randomized data subsets can support selective unlearning by retraining only a subset of the ensemble\nAchieved better validation loss and accuracy than a larger monolithic ResNet-152 model despite using fewer total parameters\nThe system supports online-style learning, where data is introduced incrementally and unlearning requests can be processed efficiently\nDemonstrated that privacy compliance and model performance need not be mutually exclusive\n\nTechnical Implementation\n\nArchitecture: Ensemble of five ResNet-18 models (11.4M parameters each)\nBaseline Comparison: Single ResNet-152 (58.5M parameters) without unlearning\nTraining Strategy:\nModels trained on chunks of CIFAR-10 in sequence\nEach model receives a random subset of each chunk\nWhen data must be unlearned, models that trained on it are marked invalid\nInvalid models are replaced and retrained on the latest available data\nEvaluation:\nValidation accuracy and loss tracked after each training phase\nVisualizations highlight ensemble progress vs. baseline\n', ""\npublished: true\ntitle: Examining Compositional Behavior in Large Language Models\ndescription: A bachelor's thesis project investigating how large language models respond to in-context subtask prompting, with a focus on model behavior and prompting strategy effectiveness.\ndate: 2023-09-24\ncategory: Research\nskills: [Python, pandas, matplotlib, HuggingFace]\nurl: https://docs.google.com/presentation/d/1E8iCQb2ft1BG55gx3ug1KR_GFp9a8woTEknGuJlETbA/edit?usp=sharing\nrepository: compositional_task_evaluation\n\nOverview\nMy bachelor's thesis, supervised by Professor Vivek Srikumar, explores the compositional capabilities of large language models (LLMs). The project investigates whether in-context examples of subtasks—rather than complete tasks—can improve model performance in compositional reasoning scenarios. The research focused on tasks involving multi-digit multiplication and word list operations, analyzed under various prompting conditions using few-shot setups.\nKey Findings\n\nSubtask-based few-shot prompts showed limited or inconsistent gains\nFlan-T5 struggled with list-based operations more than expected\nPerformance was highly sensitive to prompt format and example selection\nModels exhibited task-specific reasoning gaps, even with relevant examples\n\nTechnical Implementation\n\nIndependently implemented all experiments and research infrastructure\nManually crafted and iterated on dozens of in-context prompt formats\nDesigned a suite of compositional tasks for systematic testing\nCreated Slurm scripts for efficient job scheduling across runs\nDeveloped comparison tools to measure accuracy and stability across models\nVisualized model performance trends using matplotlib\n\nResearch Components\n\nPrompt engineering: manual experimentation with few-shot examples\nData pipeline for generating task variants and model inputs\nEvaluation framework for task-specific performance metrics\nCross-model and cross-prompt comparisons to identify behavior patterns\nSynthesis of results for academic presentation and final thesis report\n"", '\npublished: true\ntitle: Replicating Logical Inference via Neurosymbolic Computation\ndescription: Implementation and evaluation of Olausson et al.\'s LINC framework, combining LLMs with theorem provers for enhanced logical reasoning.\ndate: 2024-10-20\ncategory: Project\nskills: [Python, HuggingFace, nltk, Prover9]\n\nOverview\nThis coursework project for CS 6964 (Neuro-Symbolic Modeling) explores the intersection of neural networks and symbolic reasoning by replicating and extending Olausson et al.\'s Logical Inference via Neurosymbolic Computation (LINC) framework. The project combines the capabilities of large language models (LLMs) with traditional theorem provers to enhance logical reasoning capabilities in AI systems.\nKey Findings\n\nSuccessfully implemented the LINC pipeline and extended it with multiple LLMs\nBeam sampling significantly reduced parsing errors compared to random sampling\n""Uncertain"" outputs were common, often indicating indecision or ambiguity in reasoning\nLlama 3.1 8B Instruct achieved the best performance, comparable to GPT-3.5\nSmaller instruction-tuned models outperformed larger chat-based ones on some tasks\n\nTechnical Implementation\n\nConstructed the LINC pipeline:\nPrompt generation for candidate entailments\nFirst-order logic parsing using Prover9\nMulti-strategy evaluation with fallbacks\nIntegrated multiple LLMs including Llama 2 and Llama 3 variants\nDesigned evaluation framework for FOLIO and ProofWriter datasets\nImplemented random and beam sampling strategies for entailment generation\nAnalyzed model performance using accuracy metrics and confusion matrices\n', ""\npublished: true\ntitle: Replicating SelfExplain Architecture\ndescription: Implementation and evaluation of Rajagopal et al.'s SelfExplain architecture for interpretable text classification with concept-level explanations.\ndate: 2023-11-30\ncategory: Project\nskills: [Python, pandas, PyTorch, HuggingFace]\n\nOverview\nA coursework project for CS 6957 (NLP with Deep Learning), focused on implementing core components of Rajagopal et al.’s SelfExplain architecture. The model produces interpretable predictions by identifying constituent concepts from input sentences and measuring their influence on classification outcomes. Concepts were extracted using parse trees and embedded in the training data to enable explanation-driven classification using a RoBERTa encoder and a Locally Interpretable Layer (LIL).\nKey Achievements\n\nReplicated the SelfExplain concept attribution strategy without building a full training pipeline\nAchieved 94.8% validation accuracy on the SST2 sentiment classification task\nExtracted interpretable concept spans from parse trees and embedded them into input features\nMeasured concept influence using score differences with masked concept representations\n\nTechnical Implementation\n\nGenerated constituency parse trees to identify phrase-level concepts\nEmbedded extracted spans into model input for interpretability\nBuilt a custom PyTorch model combining RoBERTa with a Locally Interpretable Layer (LIL)\nMeasured the influence of each concept by masking and comparing prediction deltas\nEvaluated interpretability through concept-level attribution and sparsity analysis\n"", ""\npublished: true\ntitle: Package Query Experiment\ndescription: Implementation and comparison of multiple approaches for efficient package query execution, including an adaptation of the SketchRefine technique.\ndate: 2025-03-01\ncategory: Project\nskills: [Python, HuggingFace, PuLP]\n\nOverview\nThis coursework project for CS 6964 (Neuro-Symbolic Modeling) explores and implements various strategies for efficient package query execution, with a focus on optimizing query performance through innovative approaches. The project involved replicating and extending Brucato et al.'s SketchRefine technique for package queries, implementing multiple execution strategies, and comparing their performance on test data. The work focused on solving the complex problem of evaluating queries that require global conditions across result sets.\nKey Achievements\n\nImplemented three distinct query execution strategies\nAchieved 75.9% faster execution with SketchRefine approach\nSuccessfully formulated complex queries as integer linear programs (ILPs)\nDemonstrated significant performance improvements over brute-force methods\n\nTechnical Implementation\n\nDeveloped a brute-force package query executor for baseline comparison\nImplemented an ILP-based query strategy using PuLP\nAdapted and simplified the SketchRefine technique for improved performance\n""]","In the **Selective Unlearning of Training Data Without Complete Retraining** project, the ensemble model was structured as follows:

- **Architecture:** The ensemble consisted of five ResNet-18 models, each with approximately 11.4 million parameters.
- **Training Strategy:** Each model was trained on chunks of the CIFAR-10 dataset in sequence, with each model receiving a random subset of each chunk. This approach allowed for efficient training and the ability to handle unlearning requests.
- **Unlearning Process:** When data needed to be unlearned, only the models that had trained on the affected data were marked as invalid. These invalid models were then replaced and retrained on the latest available data, avoiding the need for full retraining of the entire ensemble.

This ensemble-based method demonstrated that selective unlearning could be achieved effectively while maintaining model performance and privacy compliance. The project showed better validation loss and accuracy compared to a larger monolithic ResNet-152 model, despite using fewer total parameters. 

For more detailed insights, you can refer to the project's presentation [here](https://docs.google.com/presentation/d/1ZiuXadZMVvYtrFtaREbwJNeWGyYnZwKc-gf4cEZRP-Y/edit?usp=sharing).","The architecture used was an ensemble of five ResNet-18 models, each with 11.4M parameters, which was compared against a single monolithic ResNet-152 model.",0.9999999999,1.0,0.7333333333333333,0.9445141303068564
What networking protocol was used for the TankWars game?,"['\npublished: true\ntitle: TankWars\ndescription: A real-time multiplayer tank battle game featuring client-server architecture, synchronized gameplay, and MVC design principles.\ndate: 2021-12-02\ncategory: Desktop App\nskills: [C#, WinForms, TCP, JSON]\n\n\nOverview\nTankWars is a real-time multiplayer tank battle game developed as a pair programming project for CS 3500 (Software Design I) at the University of Utah. It showcases advanced concepts in software architecture, network synchronization, and game development. The game enables players to engage in competitive tank combat, supported by a robust client-server model and real-time event handling.\nKey Features\n\nReal-time multiplayer gameplay across multiple clients\nClient-server architecture for centralized control and consistency\nSynchronized game state to ensure fairness\nInteractive tank movement and combat mechanics\nDynamic events and real-time score tracking\n\nTechnical Implementation\n\nFollowed MVC architecture for modular, maintainable code\nBuilt a TCP-based client-server model for game synchronization\nServer maintained the authoritative game state and validated player actions\nClients rendered game state and sent input events to the server\nDesigned a synchronized game loop and entity tracking system\nImplemented event-driven logic for collisions and scorekeeping\nSerialized game events and world updates using JSON\nDeveloped the game interface with WinForms for real-time rendering\n', ""\npublished: true\ntitle: Replicating Partitioned Learned Bloom Filter\ndescription: Implementation of Vaidya et al.'s PLBF for significant space efficiency improvements to traditional bloom filters.\ndate: 2024-03-23\ncategory: Project\nskills: [Python, pandas, NumPy, scikit-learn]\n\nOverview\nThis coursework project for CS 4964 (Manage Data for & with ML) replicates and evaluates the Partitioned Learned Bloom Filter (PLBF) proposed by Vaidya et al. The PLBF architecture integrates machine learning models into Bloom Filters to reduce memory usage while preserving filtering performance. This implementation explores how different classifiers impact accuracy, space efficiency, and latency across varying filter designs and datasets.\nKey Findings\n\nTraditional Bloom Filters provided the highest accuracy and fastest queries but consumed the most memory\nLearned Bloom Filters using perceptrons significantly reduced memory usage with minimal accuracy loss\nMore complex models like neural nets and ensembles improved accuracy but increased latency\nClassifier choice enables tuning of space-performance trade-offs for specific applications\n\nTechnical Implementation\n\nImplemented a traditional Bloom Filter for baseline comparison\nTrained ML models including Perceptron, Bagged Perceptron, Random Forest, and Neural Network\nPartitioned data and integrated model predictions into filtering logic\nBuilt preprocessing and evaluation pipelines for the URL and EMBER datasets\nMeasured memory usage, false positive rates, query latency, and training time\n"", ""\npublished: true\ntitle: Examining Compositional Behavior in Large Language Models\ndescription: A bachelor's thesis project investigating how large language models respond to in-context subtask prompting, with a focus on model behavior and prompting strategy effectiveness.\ndate: 2023-09-24\ncategory: Research\nskills: [Python, pandas, matplotlib, HuggingFace]\nurl: https://docs.google.com/presentation/d/1E8iCQb2ft1BG55gx3ug1KR_GFp9a8woTEknGuJlETbA/edit?usp=sharing\nrepository: compositional_task_evaluation\n\nOverview\nMy bachelor's thesis, supervised by Professor Vivek Srikumar, explores the compositional capabilities of large language models (LLMs). The project investigates whether in-context examples of subtasks—rather than complete tasks—can improve model performance in compositional reasoning scenarios. The research focused on tasks involving multi-digit multiplication and word list operations, analyzed under various prompting conditions using few-shot setups.\nKey Findings\n\nSubtask-based few-shot prompts showed limited or inconsistent gains\nFlan-T5 struggled with list-based operations more than expected\nPerformance was highly sensitive to prompt format and example selection\nModels exhibited task-specific reasoning gaps, even with relevant examples\n\nTechnical Implementation\n\nIndependently implemented all experiments and research infrastructure\nManually crafted and iterated on dozens of in-context prompt formats\nDesigned a suite of compositional tasks for systematic testing\nCreated Slurm scripts for efficient job scheduling across runs\nDeveloped comparison tools to measure accuracy and stability across models\nVisualized model performance trends using matplotlib\n\nResearch Components\n\nPrompt engineering: manual experimentation with few-shot examples\nData pipeline for generating task variants and model inputs\nEvaluation framework for task-specific performance metrics\nCross-model and cross-prompt comparisons to identify behavior patterns\nSynthesis of results for academic presentation and final thesis report\n"", ""\npublished: true\ntitle: Jordan Tan Gatsby Site\ndescription: A modern personal blog and portfolio website built with Gatsby, featuring a headless CMS integration and GraphQL data management.\ndate: 2021-06-17\ncategory: Website\nskills: [JavaScript, HTML, CSS, SASS, Gatsby, GraphQL, Strapi]\nurl: https://magnificent-trifle-352d6c.netlify.app\nrepository: jortan-site\n\n\nOverview\nA sophisticated personal website developed using Gatsby, this project demonstrates modern web development practices by combining blog functionality with portfolio presentation. It leverages Gatsby's static site generation capabilities and Strapi's headless CMS features for efficient content management and delivery.\nKey Features\n\nBlog post management\nPortfolio showcase\nHeadless CMS integration\nGraphQL data layer\nStatic site generation\nResponsive design\n\nTechnical Implementation\n\nImplemented Gatsby framework with static site generation\nIntegrated Strapi as a headless CMS for managing content\nDeveloped GraphQL queries to fetch structured data\nBuilt responsive layouts and content templates using SASS\n"", '\npublished: true\ntitle: jortan.dev\ndescription: A modern, performant personal portfolio website built with Next.js and Tailwind CSS, featuring a content-first approach using Markdown.\ndate: 2023-12-29\ncategory: Website\nskills: [TypeScript, HTML, Tailwind CSS, Next.js, Markdown, Contentlayer]\nurl: https://jortan.vercel.app\nrepository: jortan.dev\n\n\nOverview\nA personal portfolio website showcasing my projects and experiences, this site is built with modern web technologies, focusing on performance and maintainability. It represents a significant upgrade from my previous Gatsby-based portfolio, emphasizing improved performance, better developer experience, and easier content management. Built with Next.js and styled using Tailwind CSS, the site demonstrates modern web development practices while maintaining excellent performance metrics.\nKey Features\n\nServer-side rendering for optimal performance\nContent management using Markdown files\nResponsive design with Tailwind CSS\nSEO optimization\nFast page loads and smooth transitions\n\nTechnical Implementation\n\nImplemented Next.js 13+ with App Router\nSet up Contentlayer for Markdown content management\nCreated responsive layouts using Tailwind CSS\nImplemented dark mode with system preference detection\nOptimized images and assets for performance\nSet up automated deployment with Vercel\n']",The networking protocol used for the TankWars game is TCP (Transmission Control Protocol). This was part of a client-server architecture that facilitated real-time multiplayer gameplay and ensured synchronized game state across multiple clients.,The TankWars game used a TCP-based client-server model to handle game synchronization and communication between the server and multiple clients.,0.9999999999,1.0,1.0,0.9876275514819253
What technologies were used in the Drawing App?,"[""\npublished: true\ntitle: Drawing App\ndescription: An Android drawing application with user authentication, drawing tools, and social sharing features.\ndate: 2024-02-09\ncategory: Mobile App\nskills: [Kotlin, Android, Firebase, C++, Jetpack]\n\n\nOverview\nThis coursework project for CS 4530 (Mobile Application Programming) demonstrates modern Android development practices and real-time collaboration features. The Drawing App is a feature-rich Android application that allows users to create, save, and share digital artwork. Built using Kotlin and following the MVVM architecture pattern, the app offers a smooth and intuitive drawing experience with robust user authentication, image augmentation, and cloud-backed persistence.\nKey Features\n\nUser authentication and profile management via Firebase\nMultiple drawing tools: color picker and three brush types (normal, stamp, spray)\nReal-time collaborative drawing functionality\nLocal storage and Firebase-based cloud synchronization\nImage augmentation tools implemented in C++ (invert, grayscale)\nIntuitive Material Design UI with responsive performance\n\nTechnical Implementation\n\nBuilt on MVVM architecture to ensure scalability and testability\nIntegrated Firebase Authentication for secure user access\nImplemented custom drawing tools and color selection logic\nIncorporated native C++ functions for image processing\nStored drawings as image files locally and in the cloud\nSupported collaborative drawing using Firebase Realtime Database\nManaged UI state and data flow using Android Jetpack components\n\nMy Contributions\n\nSet up the project's Gradle configuration and navigation structure\nDeveloped the login screen and integrated Firebase Authentication\nImplemented functionality for saving, loading, and deleting drawings\nContributed to the foundational architecture of the application\n"", '\npublished: true\ntitle: Not Piskel\ndescription: A feature-rich sprite sheet editor built with C++ and Qt, offering animation and pixel art creation capabilities.\ndate: 2022-03-25\ncategory: Desktop App\nskills: [C++, Qt]\n\n\nOverview\nNot Piskel is a sprite sheet editor developed as a team project for CS 3505 (Software Practice II) at the University of Utah. The application combines advanced graphics programming with a user-friendly interface to support pixel artists and game developers in creating high-quality animations. Built in C++ using the Qt framework, Not Piskel delivers an intuitive and efficient workflow for editing and previewing sprite sheets.\nKey Features\n\nCustomizable canvas size and resolution\nDrawing tools: pixel brush, rectangle, fill/bucket\nFrame-based animation with timeline control\nAdjustable FPS playback for animation preview\nSaving and loading full projects\n\nTechnical Implementation\n\nDeveloped optimized drawing tools with interactive feedback\nImplemented a frame-by-frame timeline editor and playback engine\nBuilt a Qt-based UI with live preview and color palette integration\nCreated a custom file format for saving/loading multi-frame sprite sheets\nAdded undo/redo functionality using the command pattern\n\nMy Contributions\n\nBuilt pen, fill, and color tools with live canvas updates\nImplemented undo/redo system using command architecture\nDeveloped the project-saving system and file format\nDesigned the interaction logic for the timeline and preview components\n', '\npublished: true\ntitle: Parking Simulator\ndescription: An interactive educational application that teaches parking techniques through realistic physics-based simulation and scenario-based learning.\ndate: 2022-04-08\ncategory: Desktop App\nskills: [C++, Qt, Box2D]\n\n\nOverview\nParking Simulator is an educational desktop application designed to teach users various parking techniques through a realistic, physics-based simulation. Built using C++ with the Qt framework and Box2D physics engine, it combines practical tutorials with responsive vehicle control to deliver an engaging and effective learning experience.\nKey Features\n\nMultiple parking scenarios: parallel, perpendicular, and angled\nPhysics-based vehicle movement and collision detection\nInteractive tutorials with real-time visual and textual feedback\nRealistic vehicle dynamics and obstacle interaction\n\nTechnical Implementation\n\nIntegrated Box2D to simulate accurate vehicle physics and handling\nDeveloped a vehicle control system responsive to keyboard inputs\nBuilt a scenario engine for tutorial progression and difficulty scaling\nCreated a UI with Qt, including overlays and guidance elements\nImplemented a custom collision detection and feedback mechanism\n\nMy Contributions\n\nDesigned and implemented the world rendering and vehicle simulation logic\nBuilt a clear and interactive user interface using Qt\nDeveloped tutorial scenarios and level logic to support guided learning\n', ""\npublished: true\ntitle: Package Query Experiment\ndescription: Implementation and comparison of multiple approaches for efficient package query execution, including an adaptation of the SketchRefine technique.\ndate: 2025-03-01\ncategory: Project\nskills: [Python, HuggingFace, PuLP]\n\nOverview\nThis coursework project for CS 6964 (Neuro-Symbolic Modeling) explores and implements various strategies for efficient package query execution, with a focus on optimizing query performance through innovative approaches. The project involved replicating and extending Brucato et al.'s SketchRefine technique for package queries, implementing multiple execution strategies, and comparing their performance on test data. The work focused on solving the complex problem of evaluating queries that require global conditions across result sets.\nKey Achievements\n\nImplemented three distinct query execution strategies\nAchieved 75.9% faster execution with SketchRefine approach\nSuccessfully formulated complex queries as integer linear programs (ILPs)\nDemonstrated significant performance improvements over brute-force methods\n\nTechnical Implementation\n\nDeveloped a brute-force package query executor for baseline comparison\nImplemented an ILP-based query strategy using PuLP\nAdapted and simplified the SketchRefine technique for improved performance\n"", ""\npublished: true\ntitle: DeliMeals\ndescription: A feature-rich recipe management application built with Flutter, offering meal categorization, favorites, and detailed cooking instructions.\ndate: 2022-12-24\ncategory: Mobile App\nskills: [Dart, Flutter]\nrepository: delimeals_app\n\n\nOverview\nDeliMeals is a comprehensive recipe management application developed as part of Academind's Flutter & Dart course. It demonstrates modern mobile development practices and user interface design, helping users discover, organize, and follow recipes. The app provides an intuitive interface for browsing meal categories, saving favorites, and accessing detailed cooking instructions.\nKey Features\n\nMeal categorization system\nFavorite meals management\nDetailed recipe instructions\nIngredient lists\nFiltering capabilities\nResponsive UI design\n\nTechnical Implementation\n\nImplemented multi-screen navigation\nDeveloped state management system\nCreated recipe data structure\nBuilt filtering mechanism\nDesigned user interface\nImplemented favorites system\n""]","The Drawing App is an Android application that features user authentication, various drawing tools, and social sharing capabilities. Here are the key technologies used in its development:

- **Kotlin**: The primary programming language for Android development.
- **Android**: The platform for which the app was developed.
- **Firebase**: Used for user authentication and cloud synchronization.
- **C++**: Employed for implementing image augmentation tools.
- **Jetpack**: Utilized for managing UI state and data flow.

The app follows the MVVM architecture pattern, ensuring scalability and testability, and includes features like real-time collaborative drawing and a responsive Material Design UI.",The Drawing App was built with Kotlin for the Android platform and used Firebase for user authentication and real-time database capabilities. It also incorporated C++ for image augmentation and used Jetpack components for UI and state management.,0.9999999999,1.0,0.9375,0.9442082467540086
How much was the RMSE reduced in the Data-centric ML Regression project?,"['\npublished: true\ntitle: Data-centric ML Regression\ndescription: A comprehensive data engineering project focused on improving flight delay prediction through advanced data cleaning and feature engineering techniques.\ndate: 2024-03-01\ncategory: Project\nskills: [Python, pandas, scikit-learn]\n\nOverview\nThis coursework project for CS 4964 (Manage Data for & with ML) highlights the importance of data quality in machine learning workflows. The goal was to improve a regression model for flight delay prediction by applying structured data preprocessing and feature engineering to a noisy, real-world dataset.\nKey Achievements\n\nReduced RMSE from 2,049,382 to 4.9 (99.99% improvement)\nImplemented robust data cleaning pipelines\nDeveloped automated feature engineering processes\nCreated reproducible data transformation workflows\n\nTechnical Implementation\n\nDesigned and implemented data cleaning pipelines including missing value imputation and outlier handling\nEngineered features using normalization, encoding, and time-based extraction\nBuilt automated validation checks and transformation pipelines\nApplied cross-validation and correlation analysis for feature selection\n', '\npublished: true\ntitle: Selective Unlearning of Training Data Without Complete Retraining\ndescription: A research project exploring efficient machine unlearning techniques using ensemble models to protect user privacy while maintaining model performance.\ndate: 2023-06-28\ncategory: Research\nskills: [Python, PyTorch]\nurl: https://docs.google.com/presentation/d/1ZiuXadZMVvYtrFtaREbwJNeWGyYnZwKc-gf4cEZRP-Y/edit?usp=sharing\nrepository: machine_unlearning\n\nOverview\nThis NSF-sponsored research project investigates machine unlearning, the process of removing the influence of specific data points from trained machine learning models. This is essential for privacy-preserving systems where users may request their data to be deleted, and retraining a model from scratch is inefficient or impractical.\nTo address this, we propose an ensemble-based method: a collection of models, each trained on random subsets of the dataset. When unlearning is required, only the affected models are replaced, avoiding full retraining and enabling continual learning.\nKey Findings\n\nEnsemble models trained on randomized data subsets can support selective unlearning by retraining only a subset of the ensemble\nAchieved better validation loss and accuracy than a larger monolithic ResNet-152 model despite using fewer total parameters\nThe system supports online-style learning, where data is introduced incrementally and unlearning requests can be processed efficiently\nDemonstrated that privacy compliance and model performance need not be mutually exclusive\n\nTechnical Implementation\n\nArchitecture: Ensemble of five ResNet-18 models (11.4M parameters each)\nBaseline Comparison: Single ResNet-152 (58.5M parameters) without unlearning\nTraining Strategy:\nModels trained on chunks of CIFAR-10 in sequence\nEach model receives a random subset of each chunk\nWhen data must be unlearned, models that trained on it are marked invalid\nInvalid models are replaced and retrained on the latest available data\nEvaluation:\nValidation accuracy and loss tracked after each training phase\nVisualizations highlight ensemble progress vs. baseline\n', ""\npublished: true\ntitle: Replicating Partitioned Learned Bloom Filter\ndescription: Implementation of Vaidya et al.'s PLBF for significant space efficiency improvements to traditional bloom filters.\ndate: 2024-03-23\ncategory: Project\nskills: [Python, pandas, NumPy, scikit-learn]\n\nOverview\nThis coursework project for CS 4964 (Manage Data for & with ML) replicates and evaluates the Partitioned Learned Bloom Filter (PLBF) proposed by Vaidya et al. The PLBF architecture integrates machine learning models into Bloom Filters to reduce memory usage while preserving filtering performance. This implementation explores how different classifiers impact accuracy, space efficiency, and latency across varying filter designs and datasets.\nKey Findings\n\nTraditional Bloom Filters provided the highest accuracy and fastest queries but consumed the most memory\nLearned Bloom Filters using perceptrons significantly reduced memory usage with minimal accuracy loss\nMore complex models like neural nets and ensembles improved accuracy but increased latency\nClassifier choice enables tuning of space-performance trade-offs for specific applications\n\nTechnical Implementation\n\nImplemented a traditional Bloom Filter for baseline comparison\nTrained ML models including Perceptron, Bagged Perceptron, Random Forest, and Neural Network\nPartitioned data and integrated model predictions into filtering logic\nBuilt preprocessing and evaluation pipelines for the URL and EMBER datasets\nMeasured memory usage, false positive rates, query latency, and training time\n"", ""\npublished: true\ntitle: Replicating SelfExplain Architecture\ndescription: Implementation and evaluation of Rajagopal et al.'s SelfExplain architecture for interpretable text classification with concept-level explanations.\ndate: 2023-11-30\ncategory: Project\nskills: [Python, pandas, PyTorch, HuggingFace]\n\nOverview\nA coursework project for CS 6957 (NLP with Deep Learning), focused on implementing core components of Rajagopal et al.’s SelfExplain architecture. The model produces interpretable predictions by identifying constituent concepts from input sentences and measuring their influence on classification outcomes. Concepts were extracted using parse trees and embedded in the training data to enable explanation-driven classification using a RoBERTa encoder and a Locally Interpretable Layer (LIL).\nKey Achievements\n\nReplicated the SelfExplain concept attribution strategy without building a full training pipeline\nAchieved 94.8% validation accuracy on the SST2 sentiment classification task\nExtracted interpretable concept spans from parse trees and embedded them into input features\nMeasured concept influence using score differences with masked concept representations\n\nTechnical Implementation\n\nGenerated constituency parse trees to identify phrase-level concepts\nEmbedded extracted spans into model input for interpretability\nBuilt a custom PyTorch model combining RoBERTa with a Locally Interpretable Layer (LIL)\nMeasured the influence of each concept by masking and comparing prediction deltas\nEvaluated interpretability through concept-level attribution and sparsity analysis\n"", ""\npublished: true\ntitle: Examining Compositional Behavior in Large Language Models\ndescription: A bachelor's thesis project investigating how large language models respond to in-context subtask prompting, with a focus on model behavior and prompting strategy effectiveness.\ndate: 2023-09-24\ncategory: Research\nskills: [Python, pandas, matplotlib, HuggingFace]\nurl: https://docs.google.com/presentation/d/1E8iCQb2ft1BG55gx3ug1KR_GFp9a8woTEknGuJlETbA/edit?usp=sharing\nrepository: compositional_task_evaluation\n\nOverview\nMy bachelor's thesis, supervised by Professor Vivek Srikumar, explores the compositional capabilities of large language models (LLMs). The project investigates whether in-context examples of subtasks—rather than complete tasks—can improve model performance in compositional reasoning scenarios. The research focused on tasks involving multi-digit multiplication and word list operations, analyzed under various prompting conditions using few-shot setups.\nKey Findings\n\nSubtask-based few-shot prompts showed limited or inconsistent gains\nFlan-T5 struggled with list-based operations more than expected\nPerformance was highly sensitive to prompt format and example selection\nModels exhibited task-specific reasoning gaps, even with relevant examples\n\nTechnical Implementation\n\nIndependently implemented all experiments and research infrastructure\nManually crafted and iterated on dozens of in-context prompt formats\nDesigned a suite of compositional tasks for systematic testing\nCreated Slurm scripts for efficient job scheduling across runs\nDeveloped comparison tools to measure accuracy and stability across models\nVisualized model performance trends using matplotlib\n\nResearch Components\n\nPrompt engineering: manual experimentation with few-shot examples\nData pipeline for generating task variants and model inputs\nEvaluation framework for task-specific performance metrics\nCross-model and cross-prompt comparisons to identify behavior patterns\nSynthesis of results for academic presentation and final thesis report\n""]","In the Data-centric ML Regression project, Jordan achieved a remarkable reduction in RMSE from 2,049,382 to 4.9, which represents a 99.99% improvement. This project focused on enhancing flight delay prediction through advanced data cleaning and feature engineering techniques.","The project achieved a 99.99% improvement in RMSE, reducing it from 2,049,382 to 4.9 through advanced data cleaning and feature engineering.",0.9999999999,1.0,0.8,0.8564985349735731
What is Jordan's opinion on the best programming language?,[],"Jordan's specific opinion on the best programming language is not provided in the available information. However, he specializes in web development and AI, with proficiency in TypeScript, Python, and C#. These languages are often favored for their versatility and strong community support in their respective fields. If you have a specific context in mind, I can help further!",I do not have information about Jordan's personal opinions on programming languages. I can only provide details about the projects in his portfolio.,0.0,0.0,0.3333333333333333,0.0
Can you provide a link to the TankWars game?,"['\npublished: true\ntitle: TankWars\ndescription: A real-time multiplayer tank battle game featuring client-server architecture, synchronized gameplay, and MVC design principles.\ndate: 2021-12-02\ncategory: Desktop App\nskills: [C#, WinForms, TCP, JSON]\n\n\nOverview\nTankWars is a real-time multiplayer tank battle game developed as a pair programming project for CS 3500 (Software Design I) at the University of Utah. It showcases advanced concepts in software architecture, network synchronization, and game development. The game enables players to engage in competitive tank combat, supported by a robust client-server model and real-time event handling.\nKey Features\n\nReal-time multiplayer gameplay across multiple clients\nClient-server architecture for centralized control and consistency\nSynchronized game state to ensure fairness\nInteractive tank movement and combat mechanics\nDynamic events and real-time score tracking\n\nTechnical Implementation\n\nFollowed MVC architecture for modular, maintainable code\nBuilt a TCP-based client-server model for game synchronization\nServer maintained the authoritative game state and validated player actions\nClients rendered game state and sent input events to the server\nDesigned a synchronized game loop and entity tracking system\nImplemented event-driven logic for collisions and scorekeeping\nSerialized game events and world updates using JSON\nDeveloped the game interface with WinForms for real-time rendering\n', ""\npublished: true\ntitle: Replicating Partitioned Learned Bloom Filter\ndescription: Implementation of Vaidya et al.'s PLBF for significant space efficiency improvements to traditional bloom filters.\ndate: 2024-03-23\ncategory: Project\nskills: [Python, pandas, NumPy, scikit-learn]\n\nOverview\nThis coursework project for CS 4964 (Manage Data for & with ML) replicates and evaluates the Partitioned Learned Bloom Filter (PLBF) proposed by Vaidya et al. The PLBF architecture integrates machine learning models into Bloom Filters to reduce memory usage while preserving filtering performance. This implementation explores how different classifiers impact accuracy, space efficiency, and latency across varying filter designs and datasets.\nKey Findings\n\nTraditional Bloom Filters provided the highest accuracy and fastest queries but consumed the most memory\nLearned Bloom Filters using perceptrons significantly reduced memory usage with minimal accuracy loss\nMore complex models like neural nets and ensembles improved accuracy but increased latency\nClassifier choice enables tuning of space-performance trade-offs for specific applications\n\nTechnical Implementation\n\nImplemented a traditional Bloom Filter for baseline comparison\nTrained ML models including Perceptron, Bagged Perceptron, Random Forest, and Neural Network\nPartitioned data and integrated model predictions into filtering logic\nBuilt preprocessing and evaluation pipelines for the URL and EMBER datasets\nMeasured memory usage, false positive rates, query latency, and training time\n"", ""\npublished: true\ntitle: Examining Compositional Behavior in Large Language Models\ndescription: A bachelor's thesis project investigating how large language models respond to in-context subtask prompting, with a focus on model behavior and prompting strategy effectiveness.\ndate: 2023-09-24\ncategory: Research\nskills: [Python, pandas, matplotlib, HuggingFace]\nurl: https://docs.google.com/presentation/d/1E8iCQb2ft1BG55gx3ug1KR_GFp9a8woTEknGuJlETbA/edit?usp=sharing\nrepository: compositional_task_evaluation\n\nOverview\nMy bachelor's thesis, supervised by Professor Vivek Srikumar, explores the compositional capabilities of large language models (LLMs). The project investigates whether in-context examples of subtasks—rather than complete tasks—can improve model performance in compositional reasoning scenarios. The research focused on tasks involving multi-digit multiplication and word list operations, analyzed under various prompting conditions using few-shot setups.\nKey Findings\n\nSubtask-based few-shot prompts showed limited or inconsistent gains\nFlan-T5 struggled with list-based operations more than expected\nPerformance was highly sensitive to prompt format and example selection\nModels exhibited task-specific reasoning gaps, even with relevant examples\n\nTechnical Implementation\n\nIndependently implemented all experiments and research infrastructure\nManually crafted and iterated on dozens of in-context prompt formats\nDesigned a suite of compositional tasks for systematic testing\nCreated Slurm scripts for efficient job scheduling across runs\nDeveloped comparison tools to measure accuracy and stability across models\nVisualized model performance trends using matplotlib\n\nResearch Components\n\nPrompt engineering: manual experimentation with few-shot examples\nData pipeline for generating task variants and model inputs\nEvaluation framework for task-specific performance metrics\nCross-model and cross-prompt comparisons to identify behavior patterns\nSynthesis of results for academic presentation and final thesis report\n"", ""\npublished: true\ntitle: Jordan Tan Gatsby Site\ndescription: A modern personal blog and portfolio website built with Gatsby, featuring a headless CMS integration and GraphQL data management.\ndate: 2021-06-17\ncategory: Website\nskills: [JavaScript, HTML, CSS, SASS, Gatsby, GraphQL, Strapi]\nurl: https://magnificent-trifle-352d6c.netlify.app\nrepository: jortan-site\n\n\nOverview\nA sophisticated personal website developed using Gatsby, this project demonstrates modern web development practices by combining blog functionality with portfolio presentation. It leverages Gatsby's static site generation capabilities and Strapi's headless CMS features for efficient content management and delivery.\nKey Features\n\nBlog post management\nPortfolio showcase\nHeadless CMS integration\nGraphQL data layer\nStatic site generation\nResponsive design\n\nTechnical Implementation\n\nImplemented Gatsby framework with static site generation\nIntegrated Strapi as a headless CMS for managing content\nDeveloped GraphQL queries to fetch structured data\nBuilt responsive layouts and content templates using SASS\n"", '\npublished: true\ntitle: jortan.dev\ndescription: A modern, performant personal portfolio website built with Next.js and Tailwind CSS, featuring a content-first approach using Markdown.\ndate: 2023-12-29\ncategory: Website\nskills: [TypeScript, HTML, Tailwind CSS, Next.js, Markdown, Contentlayer]\nurl: https://jortan.vercel.app\nrepository: jortan.dev\n\n\nOverview\nA personal portfolio website showcasing my projects and experiences, this site is built with modern web technologies, focusing on performance and maintainability. It represents a significant upgrade from my previous Gatsby-based portfolio, emphasizing improved performance, better developer experience, and easier content management. Built with Next.js and styled using Tailwind CSS, the site demonstrates modern web development practices while maintaining excellent performance metrics.\nKey Features\n\nServer-side rendering for optimal performance\nContent management using Markdown files\nResponsive design with Tailwind CSS\nSEO optimization\nFast page loads and smooth transitions\n\nTechnical Implementation\n\nImplemented Next.js 13+ with App Router\nSet up Contentlayer for Markdown content management\nCreated responsive layouts using Tailwind CSS\nImplemented dark mode with system preference detection\nOptimized images and assets for performance\nSet up automated deployment with Vercel\n']","I don't have a specific link to the TankWars game. However, I can provide you with a brief overview of the project:

**TankWars** is a real-time multiplayer tank battle game developed as a pair programming project for CS 3500 (Software Design I) at the University of Utah. It features a client-server architecture, synchronized gameplay, and follows MVC design principles. Players engage in competitive tank combat, supported by a robust client-server model and real-time event handling.

If you need more details about the project or its features, feel free to ask!","I do not have a direct download link for the TankWars game. However, you can find more information about its implementation in the portfolio.",0.9999999999,0.5,0.8,0.0
